{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-10/cnn_lstm_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IghAKNFYaKnI"
      },
      "source": [
        "# Video Anomaly Detection \n",
        "                                                             \n",
        "<center>\n",
        "    <img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/video-anomaly.png\" height=\"267\" width=\"500\" style=\"vertical-align:middle;margin:10px 20px\"/>\n",
        "</center>\n",
        "                \n",
        "\n",
        "In this lab,we will train a convolutional LSTM autoencoder model to learn what is normal pedestrian traffic and then use it to detect unusual activities. This technique is described in this [paper](https://arxiv.org/pdf/1701.01546.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEM21coBaKnJ"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBijAMy2aKnJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import os\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as ipyImage\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg5MSie9aKnN"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "**UCSD Anomaly Detection Dataset**\n",
        "\n",
        "The UCSD Anomaly Detection Dataset is a set of video frames from a stationary camera overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. We will only use the *Peds1* subset, which contains 34 training video samples (normal scenes) and 36 testing video samples (anomalous scenes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIl8z5Y3aKnN"
      },
      "source": [
        "### Download the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmYJCwQBaKnO"
      },
      "outputs": [],
      "source": [
        "dataset_url  = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/UCSDped1.zip'\n",
        "path_to_zip = keras.utils.get_file('UCSDped1.zip', origin=dataset_url, extract=True, cache_dir='.')\n",
        "dataset_dir = os.path.join(os.path.dirname(path_to_zip), 'UCSDped1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU7oGlQ7aKnQ"
      },
      "source": [
        "After unzipping the file, you will see two subfolders: `Train` and `Test`. \n",
        "The training data (consists of 24 video clips) are in Train subfolder. Each clip is in a separate subfolder 'Train001', 'Train002', etc, and each of these subfolders contains 200 image frames. Similarly for the test data.\n",
        "\n",
        "In the code below, we are just setting up all the filepaths to be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZIu-7NQaKnQ"
      },
      "outputs": [],
      "source": [
        "# setup all the relative path\n",
        "train_dir = os.path.join(dataset_dir, 'Train')\n",
        "test_dir = os.path.join(dataset_dir, 'Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkzuMjb1aKnS"
      },
      "source": [
        "### Visualize the Train dataset\n",
        "\n",
        "Our training set contains only video scenes that are 'normal'. We will convert the image frames to a 'video' (actually as animated gif) for easier viewing. The video consists of 200 frames. From the left navigation panel, you will see that a gif file called <train_sample_folder_name>.gif has been created, e.g. Train034.gif."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just define a util function to create gif from series of images."
      ],
      "metadata": {
        "id": "_B91NfpH8BC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gif(image_folder, output_file, img_type='png'):\n",
        "    # Create the frames\n",
        "    frames = []\n",
        "\n",
        "    # files need to be sorted from 1...n so that the video is played in correct sequence\n",
        "    imgs = sorted(glob.glob(f'{image_folder}/*.{img_type}'))\n",
        "    for i in imgs:\n",
        "        new_frame = Image.open(i)\n",
        "        frames.append(new_frame)\n",
        "    \n",
        "    # Save into a GIF file that loops forever\n",
        "    frames[0].save(output_file, format='gif',\n",
        "                append_images=frames[1:],\n",
        "                save_all=True,\n",
        "                duration=120, loop=0)\n"
      ],
      "metadata": {
        "id": "App3w8bO8IhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmgBezKRaKnS"
      },
      "outputs": [],
      "source": [
        "# You can change the following train_sample_folder to another folder to view other clips\n",
        "train_sample_folder = 'Train034' \n",
        "image_folder = os.path.join(train_dir, train_sample_folder)\n",
        "gif_filename = train_sample_folder + '.gif' \n",
        "create_gif(image_folder, gif_filename, img_type='tif')\n",
        "with open(gif_filename,'rb') as file:\n",
        "    display(ipyImage(file.read(), format='png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZx0EKg2aKnY"
      },
      "source": [
        "## Visualize the Test dataset\n",
        "\n",
        "Let us visualize the video frames from the test dataset folder Test024, as an animated gif. You should be able to see some anomalous event (e.g. a small cart driven on the walkway) in the animated gif you create. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT8lbWc3aKnY"
      },
      "outputs": [],
      "source": [
        "# set the test sample folder to folder of Test001 and set the image_folder accordingly\n",
        "test_sample_folder = 'Test024' \n",
        "image_folder = os.path.join(test_dir, test_sample_folder) \n",
        "\n",
        "create_gif(image_folder, gif_filename, img_type='tif')\n",
        "\n",
        "with open(gif_filename,'rb') as file:\n",
        "    display(ipyImage(file.read(), format='png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1KkcvYnaKna"
      },
      "source": [
        "### Prepare Training Dataset\n",
        "\n",
        "As we will be using a convolutional LSTM to learn to reconstruct the video frames, we will prepare our data as follows: \n",
        "\n",
        "1. Divide the training video frames into temporal sequences, each of size 10 using the sliding window technique.\n",
        "2. Resize each frame to `IMG_HEIGHT Ã— IMG_WIDTH` to ensure that input images have the same resolution.\n",
        "3. Scale the pixels values between 0 and 1 by dividing each pixel by 255.\n",
        "\n",
        "To increase the number of samples available, data augmentation in the temporal dimension is done by concatenating\n",
        "frames with stride-1, stride-2, and stride-3, as suggested in the [paper](https://arxiv.org/pdf/1701.01546.pdf). For example, the first stride-1 sequence is made up of frames `{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}`, whereas the first\n",
        "stride-2 sequence contains frames `{1, 3, 5, 7, 9, 11, 13, 15, 17, 19}`, and\n",
        "stride-3 sequence would contains frames `{1, 4, 7, 10, 13, 16, 19, 22, 25,\n",
        "28}`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_clips(stride, frames_list, sequence_length):\n",
        "   \n",
        "    augmented_clips = []\n",
        "    sz = len(frames_list)\n",
        "    img_height, img_width = frames_list[0].shape[0], frames_list[0].shape[1]\n",
        "    clip = np.zeros(shape=(sequence_length, img_height, img_width, 1))\n",
        "    cnt = 0\n",
        "    for start in range(0, stride):\n",
        "        for i in range(start, sz, stride):\n",
        "            clip[cnt, :, :, 0] = frames_list[i]\n",
        "            cnt = cnt + 1\n",
        "            if cnt == sequence_length:\n",
        "                augmented_clips.append(np.copy(clip))\n",
        "                cnt = 0\n",
        "    return augmented_clips\n",
        "\n",
        "\n",
        "def prepare_dataset(fileset, img_height, img_width, strides=1, batch_size=1, sequence_length=10):\n",
        "    \n",
        "    clips = []\n",
        "    all_frames = []\n",
        "    # important to sort the files as the video sequence must be in the correct order\n",
        "    filepaths = sorted(glob.glob(fileset))\n",
        "    \n",
        "    for fpath in filepaths:\n",
        "        img = Image.open(fpath).resize((img_height, img_width))\n",
        "        img = np.array(img, dtype=np.float32) / 255.0\n",
        "        all_frames.append(img)\n",
        "    \n",
        "    for stride in range(1, 1+strides):\n",
        "        clips.extend(augment_clips(stride=stride, \n",
        "                                   frames_list=all_frames,\n",
        "                                   sequence_length=sequence_length))\n",
        "    \n",
        "\n",
        "    # convert the list to 4D numpy array\n",
        "    clips = np.stack(clips, axis=0)\n",
        "\n",
        "    # Y = X for autoencoder\n",
        "    dataset = tf.data.Dataset.from_tensor_slices( (clips, clips) ).batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "    \n"
      ],
      "metadata": {
        "id": "3gKsaEUi1qgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icgrkb_5aKnb"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT=128\n",
        "IMG_WIDTH=128\n",
        "SEQUENCE_LENGTH=10\n",
        "BATCH_SIZE=4\n",
        "\n",
        "train_fileset = os.path.join(train_dir, '*/*.tif')\n",
        "train_dataset = prepare_dataset(train_fileset,\n",
        "                                IMG_HEIGHT, \n",
        "                                IMG_WIDTH, \n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                strides=2,\n",
        "                                sequence_length=SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the shape of one of the sample. Note that this is a 5D tensor, where the axis 1 represents the time-steps, and axis 2 and axis 3 represents the image heigth and width."
      ],
      "metadata": {
        "id": "Rgy3Vh0txglY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrRhwTOvaKnc"
      },
      "outputs": [],
      "source": [
        "sample, _ = next(train_dataset.as_numpy_iterator())\n",
        "print(sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDX8ES3raKne"
      },
      "source": [
        "### Building the Autoencoder Model\n",
        "\n",
        "\n",
        "![autoencoder](https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/iti107/resources/convlstm_autoencoder.png)\n",
        "\n",
        "\n",
        "The spatial encoder (the Conv2D layers) takes in one\n",
        "image frame at each timestep, and pass the output to the temporal encoder (the LSTM layers) for each timestep, for T (e.g., 10) timesteps. The decoders mirror the encoders to reconstruct the video\n",
        "volume. To apply Conv2D for T timesteps, we use the TimeDistributed wrapper provided by Keras API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fotLE0L2aKne"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "\n",
        "    inputs = keras.Input(shape=(SEQUENCE_LENGTH, IMG_HEIGHT, IMG_HEIGHT, 1))\n",
        "    conv = keras.layers.Conv2D(128, (11,11), strides=4, padding='same')\n",
        "    x = keras.layers.TimeDistributed(conv)(inputs)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "    conv = keras.layers.Conv2D(64, (5, 5), strides=2, padding=\"same\")\n",
        "    x = keras.layers.TimeDistributed(conv)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    x = keras.layers.ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "    x = keras.layers.ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "    x = keras.layers.ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    conv = keras.layers.Conv2DTranspose(64, (5, 5), strides=2, padding=\"same\")\n",
        "    x = keras.layers.TimeDistributed(conv)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    conv = keras.layers.Conv2DTranspose(128, (11, 11), strides=4, padding=\"same\")\n",
        "    x = keras.layers.TimeDistributed(conv)(x)\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    conv = keras.layers.Conv2D(1, (11, 11), activation=\"sigmoid\", padding=\"same\")\n",
        "    outputs = keras.layers.TimeDistributed(conv)(x)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pltQNVWJaKno"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-5CdI79aKnp"
      },
      "source": [
        "As we aims to minimise the reconstruction error, one appropriate loss function to use is the Mean Squared Error (MSE).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy0UFggJaKnq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
        "        optimizer=tf.keras.optimizers.Adam(lr=1e-4, decay=1e-5, epsilon=1e-5),\n",
        "        metrics=['mse'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5noepEtxaKnr"
      },
      "source": [
        "Let's the training begin!! This might take a while so *sit back, relax and wait!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRiJD4fVaKns"
      },
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "\n",
        "model.fit(train_dataset, epochs=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"anomaly_detector\")"
      ],
      "metadata": {
        "id": "kBIhiVUQujw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also download the model that we have trained if you cannot wait for the training to finish.  Just uncomment the following lines of codes:"
      ],
      "metadata": {
        "id": "WumENDaM4Ssp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/anomaly_detector.zip\n",
        "# !unzip anomaly_detector.zip"
      ],
      "metadata": {
        "id": "1sptl9DDxw3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpkcJZ1iaKn4"
      },
      "source": [
        "### Prepare Testing Dataset\n",
        "\n",
        "Now we will create a test dataset that we can use to test the trained auto-encoder. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_sequence(fileset, img_height, img_width, sequence_length): \n",
        "\n",
        "    filepaths = sorted(glob.glob(fileset))\n",
        "    img_array = np.zeros((len(filepaths), img_height, img_width, 1)).astype(np.float32)\n",
        "\n",
        "    for i, fpath in enumerate(filepaths):\n",
        "        im = Image.open(fpath)\n",
        "        im = im.resize((img_height, img_width))\n",
        "        img_array[i, :, :, 0] = np.array(im)/255.0\n",
        "\n",
        "    sz = img_array.shape[0] - sequence_length + 1\n",
        "    sequences = np.zeros((sz, sequence_length, img_height, img_width, 1))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((sequence_length, img_height, img_width, 1))\n",
        "        for j in range(0, sequence_length):\n",
        "            clip[j] = img_array[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "HJi7dNyuDVpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR2BnJ16aKn4"
      },
      "outputs": [],
      "source": [
        "test_fileset = os.path.join(test_dir, 'Test024/*.tif')\n",
        "test_sequence = prepare_test_sequence(test_fileset,\n",
        "                                IMG_HEIGHT, \n",
        "                                IMG_WIDTH, \n",
        "                                sequence_length=SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sequence.shape)"
      ],
      "metadata": {
        "id": "RYNw3jG39qFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFJ-43_JaKn6"
      },
      "source": [
        "#### Abnormality scores of video sequences\n",
        "\n",
        "The following codes feeds the sequences of video frames from the test video to reconstruct the video sequence. We then compute the reconstruction losses and use that to compute the normalized abnormality scores of the video sequence.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('anomaly_detector')"
      ],
      "metadata": {
        "id": "EQx4BRw4-Pww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_sequences = model.predict(test_sequence,batch_size=4)"
      ],
      "metadata": {
        "id": "LsExwKXiGAwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(test_sequence[i],reconstructed_sequences[i])) for i in range(0, test_sequence.shape[0])])\n",
        "sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)"
      ],
      "metadata": {
        "id": "5n3oDhhQGxgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sa)\n",
        "plt.ylabel('anomaly score Sa(t)')\n",
        "plt.xlabel('frame t')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0eF6p2F0GDrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_folder = 'anom_plots'\n",
        "\n",
        "def plot_anomaly_scores(original, reconstructed, scores, max_score, frame_num):\n",
        "\n",
        "    if not os.path.exists(tmp_folder):\n",
        "        os.mkdir(tmp_folder)\n",
        "    plt.ioff()\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "   \n",
        "    x = np.arange(0, 200)\n",
        "    y = np.zeros(200)\n",
        "\n",
        "    # show original image\n",
        "    fig.add_subplot(131)\n",
        "    plt.title(f'frame {frame_num}')\n",
        "    plt.set_cmap('gray')\n",
        "    plt.imshow(original)\n",
        "\n",
        "    fig.add_subplot(132)\n",
        "    plt.title(f'frame {frame_num}')\n",
        "    plt.set_cmap('gray')\n",
        "    plt.imshow(reconstructed)\n",
        "\n",
        "    fig.add_subplot(133)\n",
        " \n",
        "    plt.ylim(0, max_score + 0.1)\n",
        "    plt.title('anomaly scores')\n",
        "    x1 = np.arange(0, 200)\n",
        "    y1 = np.zeros(200)\n",
        "    plt.plot(x1, y1)\n",
        "    plt.plot(scores)\n",
        "\n",
        "    fig.savefig(os.path.join(tmp_folder, '{:0>3d}.png'.format(frame_num)))\n",
        "\n",
        "    plt.ioff()\n",
        "    plt.close()\n",
        "  \n",
        "    \n",
        "def create_video_animation(scores, orig_sequences, reconstructed_sequences,gif_file):\n",
        "\n",
        "    anom_scores = []\n",
        "    counter = 0\n",
        "    num_frames = orig_sequences.shape[0]\n",
        "    max_score = np.max(scores)\n",
        "\n",
        "    for frame_num in tqdm(range(num_frames)):\n",
        "        anom_scores.append(scores[frame_num])\n",
        "        # display the last frame of each video sequence\n",
        "        plot_anomaly_scores(orig_sequences[frame_num, 9, :, :, 0], \n",
        "                            reconstructed_sequences[frame_num, 9, :, :, 0],\n",
        "                            anom_scores, \n",
        "                            max_score, \n",
        "                            frame_num)\n",
        "     \n",
        "    \n",
        "    create_gif(tmp_folder, gif_file)"
      ],
      "metadata": {
        "id": "jXJFFckcHXza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZfbfK6XaKn6"
      },
      "outputs": [],
      "source": [
        "create_video_animation(sa, test_sequence, reconstructed_sequences, \"anomaly.gif\")\n",
        "with open('anomaly.gif','rb') as file:\n",
        "    display(ipyImage(file.read(), format='png', embed=True))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cnn_lstm_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}