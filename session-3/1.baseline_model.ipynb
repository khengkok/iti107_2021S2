{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "name": "1.baseline_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-3/1.baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUYzI2L_302m"
      },
      "source": [
        "# Baseline model\n",
        "\n",
        "Welcome to this week's programming exercise. In this exercise, we will be training a model to recognise if an image depicts positive (e.g. happy, pleasant, beautiful) or negative (e.g. sad, angry, death, etc) emotion . We will first train a baseline model without using transfer learning. The dataset is a collection of around 1600 images from Flickr, and labelled with Positive or Negative label. We only apply data augmentation to our training set. In the next exercise, we will use transfer learning technique to train another model and compare the performance of both.\n",
        "\n",
        "At the end of this exercise, you will be able to: \n",
        "- apply data augmentation to your training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4cHqTVKI302n"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBSuHCv6302o"
      },
      "source": [
        "## Download the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euo2CmNr302o"
      },
      "source": [
        "dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/intel_emotions_dataset.zip'\n",
        "path_to_zip = keras.utils.get_file('intel_emotions_dataset.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "print(path_to_zip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci54Ll9h302p"
      },
      "source": [
        "The zip file will be expanded into two subfolders, 'Positive' and 'Negative', containing images that evokes positive emotions and negative emotions respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "K5FW5Qjn302p"
      },
      "source": [
        "dataset_dir = os.path.dirname(path_to_zip)\n",
        "print(dataset_dir)\n",
        "pos_path = os.path.join(dataset_dir, 'Positive')\n",
        "neg_path = os.path.join(dataset_dir, 'Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tmP_F9w302q"
      },
      "source": [
        "### Visualizing sample images\n",
        "\n",
        "We randomly select `n_examples` and display them.\n",
        "\n",
        "**WARNING**: Some of the images may be too graphic and offensive. Please feel free to skip the following two cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeSpdZUp302q"
      },
      "source": [
        "n_examples = 5\n",
        "np.random.seed(42)\n",
        "positive_expamples = np.random.choice(os.listdir(pos_path), size=n_examples, replace=False)\n",
        "negative_expamples = np.random.choice(os.listdir(neg_path), size=n_examples, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ByupKpMl302r"
      },
      "source": [
        "plt.figure(figsize=(5, n_examples * 2))\n",
        "for i in range(n_examples):\n",
        "    plt.subplot(n_examples, 2, i * 2 + 1)\n",
        "    img = keras.utils.load_img(os.path.join(pos_path, positive_expamples[i]))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    if i == 0:\n",
        "        plt.title(\"Positive\", fontsize=18)\n",
        "    plt.subplot(n_examples, 2, i * 2 + 2)\n",
        "    img = keras.utils.load_img(os.path.join(neg_path, negative_expamples[i]))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    if i == 0:\n",
        "        plt.title(\"Negative\", fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sJQMMDN302s"
      },
      "source": [
        "## Create train and validation dataset\n",
        "\n",
        "We will use the `tf.keras.preprocessing.image_dataset_from_directory` to generate tf.data.Dataset from the data folder. Feel free to adjust the batch_size to the maximum without incurring OOM (out-of-memory) error. GPU usually have limited memory. We also use a smaller image size (128,128) to speed up our training. Although `label_mode` is not required to be specified (and can be infered from the number of subfolders), we specifically set the `label_mode='binary'`, in case our datasets folder contains more than 2 subfolders, as sometimes jupyter notebook will generate a hidden folder called '.ipynb_checkpoints, and keras may think there are 3 different labels. By setting the label_mode to binary allows us to specifically detect this kind of issues. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5yNk5ZP302s"
      },
      "source": [
        "batch_size = 8\n",
        "image_size = (128,128)\n",
        "\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "BHHElA4I302s"
      },
      "source": [
        "# Print the class names \n",
        "print(val_ds.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsPXl6HHDiv7"
      },
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlTyrF7U302t"
      },
      "source": [
        "## Data Augmentation \n",
        "\n",
        "Since tensorflow 2.2, Keras introduces new types of layers for doing image data augmentation, such as Random Cropping, Random Flipping, etc. Previously, we have to depend on ImageDataGenerator() (which is a lot slower) to do so. Before tensorflow 2.6, they are available as experimental layers (available in the tf.keras.layers.experimental.preprocessing package), but has been officially supported from tensorflow 2.6 onwards (i.e. available as part of the tf.keras.layers). \n",
        "\n",
        "In the code below, we create a Sequential model to add the image augmentation layer: `RandomRotation()`. The value `0.3` refers to the maximum rotation angle in both clock-wise and anti-clockwise direction. You can find out more info from the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xogFr782302t"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomRotation(0.3),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYFXGleG302t"
      },
      "source": [
        "To see the effects of data augmentation, let us apply our data_augmentation layer to a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5rIfje6h-Q"
      },
      "source": [
        "images, _ = next(train_ds.take(1).as_numpy_iterator())\n",
        "sample_image = images[0]/255.\n",
        "plt.imshow(sample_image)\n",
        "sample_image = tf.expand_dims(sample_image, 0)\n",
        "print(sample_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH2QsTSf6uLU"
      },
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "for i in range(8):\n",
        "    augmented_image = data_augmentation(sample_image)\n",
        "    ax = plt.subplot(2, 4, i + 1)\n",
        "    plt.imshow(augmented_image[0])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naR0Uuz6302t"
      },
      "source": [
        "**Exercise 1:**\n",
        "\n",
        "Modify the code above to add in Random Contrast and Random Cropping. Choose the appropriate values for the contrast and cropping factor.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "    \n",
        "if tf.version.VERSION >= '2.6.0':\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            layers.RandomRotation(0.3),\n",
        "            layers.RandomContrast(0.8),\n",
        "            layers.RandomZoom(0.8)\n",
        "        ]\n",
        "    )\n",
        "else: \n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            layers.experimental.preprocessing.RandomRotation(0.3),\n",
        "            layers.experimental.preprocessing.RandomContrast(0.8),\n",
        "            layers.experimental.preprocessing.RandomZoom(0.8),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    \n",
        "```\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMd5mRMP302u"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Previously we have built the mini-Xception network and it works well on our small cats and dogs dataset.  We will apply the same network for this more challenging emotions dataset and see if data augmentation helps.\n",
        "\n",
        "The following codes are same as previous xception network that you have coded. \n",
        "\n",
        "**Exercise 2:**\n",
        "\n",
        "Modify the code in `make_model()` to apply data augmention layers you have created earlier. Where should you place your augmentation layer?  \n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "def make_model(input_shape, num_classes): \n",
        "    inputs = keras.Input(shape=input_shape)    \n",
        "    \n",
        "    ## Add your augmentation layers here !! \n",
        "    x = data_augmentation(inputs) \n",
        "\n",
        "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "\n",
        "    ## the rest of the codes.... \n",
        "    \n",
        "    return keras.Model(inputs, outputs)    \n",
        "```\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "80nnDHCC302u"
      },
      "source": [
        "def xception_block(x, depth): \n",
        "\n",
        "    skip_connection = x\n",
        "    \n",
        "    x = keras.layers.SeparableConv2D(depth, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "    x = keras.layers.SeparableConv2D(depth, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "    residual = keras.layers.Conv2D(depth, 1, strides=2, padding=\"same\")(\n",
        "        skip_connection\n",
        "    )\n",
        "    x = keras.layers.add([x, residual])  # Add back residual\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "    \n",
        "    return x # Set aside next residual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qb0svEo302u"
      },
      "source": [
        "## TODO: Modify the code to add data augmentation\n",
        "\n",
        "def make_model(input_shape, num_classes): \n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs) \n",
        "    x = keras.layers.Rescaling(1.0 / 255)(inputs)\n",
        "\n",
        "    x = keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "    x = keras.layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "    \n",
        "    # our xception blocks\n",
        "    for size in [128, 256, 512, 728]:\n",
        "        # Code here\n",
        "        x = xception_block(x, size)\n",
        "    \n",
        "    x = keras.layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation(\"relu\")(x)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    outputs = keras.layers.Dense(units, activation=activation)(x)\n",
        "    \n",
        "    return keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yQ6B-Az302u"
      },
      "source": [
        "model = make_model(input_shape= image_size + (3,), num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWm38Ol302v"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Let's train our new model with the data augmentation layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yicw3NhT302v"
      },
      "source": [
        "def create_tb_callback(): \n",
        "\n",
        "    import os\n",
        "    \n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "        \n",
        "        import time\n",
        "        \n",
        "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "        return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bestcheckpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "# compile our model with loss and optimizer \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds, \n",
        "    epochs=15, \n",
        "    validation_data=val_ds,\n",
        "    callbacks=[model_checkpoint_callback, create_tb_callback()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMHW1w_P302v"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaU14-gf302v"
      },
      "source": [
        "As you can see from the plot, our model starts to overfit from epoch 10 onwards and the validation accuracy fluctuates around 56-57%. Even with data augmentation, the augmented images are still heavily correlated, since they come from a small number of original images -- we cannot produce new information, we can only remix existing information. As next step to improve our accuracy on this problem, we will have to leverage transfer learning using pre-trained model, which will be the focus of next exercises."
      ]
    }
  ]
}