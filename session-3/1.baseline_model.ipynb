{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-7/baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "Welcome to this week's programming exercise. In this exercise, we will be training a model to recognise if an image depicts positive (e.g. happy, pleasant, beautiful) or negative (e.g. sad, angry, death, etc) emotion . We will first train a baseline model without using transfer learning. The dataset is a collection of around 1600 images from Flickr, and labelled with Positive or Negative label. We only apply data augmentation to our training set. In the next exercise, we will use transfer learning technique to train another model and compare the performance of both.\n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- apply data augmentation to your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/intel_emotions_dataset.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('intel_emotions_dataset.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
    "print(path_to_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip file will be expanded into two subfolders, 'Positive' and 'Negative', containing images that evokes positive emotions and negative emotions respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = os.path.dirname(path_to_zip)\n",
    "print(dataset_dir)\n",
    "pos_path = os.path.join(dataset_dir, 'Positive')\n",
    "neg_path = os.path.join(dataset_dir, 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sample images\n",
    "\n",
    "We randomly select `n_examples` and display them.\n",
    "\n",
    "**WARNING**: Some of the images may be too graphic and offensive. Please feel free to skip the following two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "np.random.seed(42)\n",
    "positive_expamples = np.random.choice(os.listdir(pos_path), size=n_examples, replace=False)\n",
    "negative_expamples = np.random.choice(os.listdir(neg_path), size=n_examples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, n_examples * 2))\n",
    "for i in range(n_examples):\n",
    "    plt.subplot(n_examples, 2, i * 2 + 1)\n",
    "    img = keras.utils.load_img(os.path.join(pos_path, positive_expamples[i]))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.title(\"Positive\", fontsize=18)\n",
    "    plt.subplot(n_examples, 2, i * 2 + 2)\n",
    "    img = keras.utils.load_img(os.path.join(neg_path, negative_expamples[i]))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.title(\"Negative\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation dataset\n",
    "\n",
    "We will use the tf.keras.preprocessing.image_dataset_from_directory to generate tf.data.Dataset from the data folder. Feel free to adjust the batch_size to the maximum without incurring OOM (out-of-memory) error. GPU usually have limited memory. We also use a smaller image size (128,128) to speed up our training. Although `label_mode` is not required to be specified (and can be infered from the number of subfolders), we specifically set the `label_mode='binary'`, in case our datasets folder contains more than 2 subfolders, as sometimes jupyter notebook will generate a hidden folder called '.ipynb_checkpoints, and keras may think there are 3 different labels. By setting the label_mode to binary allows us to specifically detect this kind of issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "image_size = (128,128)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary'\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the class names \n",
    "print(val_ds.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation \n",
    "\n",
    "Since tensorflow 2.2, Keras introduces new types of layers for doing image data augmentation, such as Random Cropping, Random Flipping, etc. Previously, we have to depend on ImageDataGenerator() (which is a lot slower) to do so. Before tensorflow 2.6, they are available as experimental layers (available in the experimental package), but has been officially supported from tensorflow 2.6 onwards (available as part of the tensorflow.keras.layers).\n",
    "\n",
    "In the code below, we will check the tensorflow version and instantiate the correct layer depending on the version. We only one RandomRotation layer in the example below. The value `0.3` refers to the maximum rotation angle in both clock-wise and anti-clockwise direction. You can find out more info from the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.version.VERSION >= '2.6.0':\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomRotation(0.3),\n",
    "        ]\n",
    "    )\n",
    "else: \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effects of data augmentation, let us apply our data_augmentation layer to a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = keras.utils.load_img(os.path.join(pos_path, positive_expamples[4]))\n",
    "# plt.imshow(image)\n",
    "image = tf.expand_dims(image, 0)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(19):\n",
    "    augmented_image = data_augmentation(image)\n",
    "    ax = plt.subplot(5, 4, i + 1)\n",
    "    plt.imshow(augmented_image[0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Modify the code above to add in Random Contrast and Random Cropping. Choose the appropriate values for the contrast and cropping factor.\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "if tf.version.VERSION >= '2.6.0':\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomRotation(0.3),\n",
    "            layers.RandomContrast(0.8),\n",
    "            layers.RandomZoom(0.8)\n",
    "        ]\n",
    "    )\n",
    "else: \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "            layers.experimental.preprocessing.RandomContrast(0.8),\n",
    "            layers.experimental.preprocessing.RandomZoom(0.8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Previously we have built the mini-Xception network and it works well on our small cats and dogs dataset.  We will apply the same network for this more challenging emotions dataset and see if data augmentation helps.\n",
    "\n",
    "The following codes are same as previous xception network that you have coded. \n",
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Modify the code in `make_model()` to apply data augmention layers you have created earlier. Where should you place your augmentation layer?  \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "def make_model(input_shape, num_classes): \n",
    "    inputs = keras.Input(shape=input_shape)    \n",
    "    \n",
    "    ## Add your augmentation layers here !! \n",
    "    x = data_augmentation(inputs) \n",
    "\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "\n",
    "    ## the rest of the codes.... \n",
    "    \n",
    "    return keras.Model(inputs, outputs)    \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers \n",
    "\n",
    "def xception_block(x, depth): \n",
    "\n",
    "    skip_connection = x\n",
    "    \n",
    "    x = layers.SeparableConv2D(depth, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(depth, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    residual = layers.Conv2D(depth, 1, strides=2, padding=\"same\")(\n",
    "        skip_connection\n",
    "    )\n",
    "    x = layers.add([x, residual])  # Add back residual\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    return x # Set aside next residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Modify the code to add data augmentation\n",
    "\n",
    "def make_model(input_shape, num_classes): \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs) \n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    # our xception blocks\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        # Code here\n",
    "        x = xception_block(x, size)\n",
    "    \n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape= image_size + (3,), num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Because data is drawn endlessly from generator, you need to tell Keras model how many samples to draw from generator before declaring an epoch is over. This is the the role of `steps_per_epoch`. \n",
    "\n",
    "Below, we set the `steps_per_epoch` to be equal to 'number of samples/batch size'. However, this is kind of arbitrary, and it does not mean the generator will return all the images available in the directory. For example, if we have 100 different images in the directory and our batch size is 10, our steps_per_epoch = 100/10, i.e. 10. However, after 10 steps of 10 images, for a total of 100 generated images, not all the original 100 images in the directory will be used. This is because ImageDataGenerator randomly transforms the images, and you may get two slightly transformed versions of the same image, instead of 2 different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tb_callback(): \n",
    "\n",
    "    import os\n",
    "    \n",
    "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "    def get_run_logdir():    # use a new directory for each run\n",
    "        \n",
    "        import time\n",
    "        \n",
    "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "        return os.path.join(root_logdir, run_id)\n",
    "\n",
    "    run_logdir = get_run_logdir()\n",
    "\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    return tb_callback\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"bestcheckpoint\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# compile our model with loss and optimizer \n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, epochs=20, \n",
    "    validation_data=val_ds,\n",
    "    callbacks=[model_checkpoint_callback, create_tb_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plot, the validation accuracy fluctuates around 55% point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report on Test Data \n",
    "\n",
    "By right, you should have allocated some data as test set for your test model. Since our data is pretty small, we did not. But for the sake of having better idea how our model is faring on each class, let's just use our validation data for getting some hard numbers :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(val_ds)\n",
    "print(len(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate([y for x, y in val_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, y_preds > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.concatenate([y for x, y in train_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(train_labels.flatten().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
