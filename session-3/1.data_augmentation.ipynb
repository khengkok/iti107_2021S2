{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-3/1.data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZIgnFFEv5t",
        "tags": []
      },
      "source": [
        "# Lab Exercise: Data Augmentation\n",
        "\n",
        "In our previous exercise with the cats and dogs dataset, our validation accuracy stalls at 75%. Because we only have relatively few training samples (2400), overfitting is going to be our number one concern. Overfitting is caused by having too few samples to learn from, rendering our model to be unable to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. This helps the model get exposed to more aspects of the data and generalize better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQgbpqHw-vXy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itUM9lsYcdOp"
      },
      "source": [
        "## Create train and validation dataset\n",
        "\n",
        "We will go ahead and download the same 'cats and dogs' dataset, and setup the training and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuLzfHcfcdOq"
      },
      "source": [
        "import os \n",
        "\n",
        "dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
        "tf.keras.utils.get_file(origin=dataset_URL, extract=True, cache_dir='.')\n",
        "dataset_folder = os.path.join('datasets', 'cats_and_dogs_subset')\n",
        "\n",
        "batch_size = 16\n",
        "image_size = (128,128)\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9YQpQ8-vYE"
      },
      "source": [
        "## Using Keras Data Augmentation layer\n",
        "\n",
        "\n",
        "Since tensorflow 2.2, Keras has introduced new types of layers for doing image data augmentation, such as Random Cropping, Random Flipping, etc. Previously, we have to depend on `ImageDataGenerator` (which is a lot slower) to do so. Before tensorflow 2.6, they are available as experimental layers (in the `tf.keras.layers.experimental.preprocessing` package), but has been officially supported from tensorflow 2.6 onwards (i.e. available as part of the `tf.keras.layers`).\n",
        "\n",
        "In the code below, we will check the tensorflow version and instantiate the correct layer depending on the version. We use only one RandomRotation layer in the example below. The value `0.3` refers to the maximum rotation angle in both clock-wise and anti-clockwise direction. You can find out more info from the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbAfiBdu-vYE"
      },
      "source": [
        "if tf.version.VERSION >= '2.6.0':\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.RandomRotation(0.3)\n",
        "        ]\n",
        "    )\n",
        "else: \n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.experimental.preprocessing.RandomRotation(0.3)\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chGB_tp0OmVI"
      },
      "source": [
        "\n",
        "To see the effects of data augmentation, let us apply our data_augmentation layer to a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGJAgFv7OojQ"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "images, _ = next(train_ds.take(1).as_numpy_iterator())\n",
        "sample_image = images[0]/255.\n",
        "plt.imshow(sample_image)\n",
        "sample_image = tf.expand_dims(sample_image, 0)\n",
        "print(sample_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gtBzOgkP3ng"
      },
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "for i in range(8):\n",
        "    augmented_image = data_augmentation(sample_image)\n",
        "    ax = plt.subplot(2, 4, i + 1)\n",
        "    plt.imshow(augmented_image[0])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qiDkZc-vYF"
      },
      "source": [
        "**Exercise 1:**\n",
        "\n",
        "Modify `data_augmention` above to add in [Random Flipping](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip) and [Random Zoom](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom). Choose the appropriate values for the flipping and cropping heights/widths. \n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomRotation(0.3),\n",
        "        keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "        keras.layers.RandomZoom(0.2)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLPiFmw-vYF"
      },
      "source": [
        "## Complete the code \n",
        "\n",
        "data_augmentation = ??\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JmRbOsBwN9H"
      },
      "source": [
        "### Using data augmentation within the data pipeline\n",
        "\n",
        "There are two ways of using the data augmentation. We can use it as part of the model, and if we are training our model on GPU, it can take advantage of GPU to speed up processing of the images. Or we can use it as part of the data pipeline, which means the image processing is done using CPU, which will be slower. We can speed up things a bit by using caching, prefetching and also do parallel processing using multiple cpu cores.\n",
        "\n",
        "Comment out the codes below if you want to apply data augmentation as part of the data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgIFZqnqhoJ3"
      },
      "source": [
        "# train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
        "#                         num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I6r5H9B-vYF"
      },
      "source": [
        "**Exercise 2:**\n",
        "\n",
        "Modify `make_model()` to apply data augmention layers you have created earlier. Where should you place your augmentation layer?\n",
        "\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "def make_model():\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(128, 128, 3)))\n",
        "    ## Use your augmentation layer here \n",
        "    model.add(data_augmentation)\n",
        "    \n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    ...\n",
        "    ...\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfk0cqS2YHYj"
      },
      "source": [
        "def make_model():\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        " \n",
        "    model.add(keras.layers.Input(shape=image_size+(3,)))\n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(32, (3, 3)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(64, (3, 3)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(128, (3, 3)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(128, (3, 3)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUsyk60w8Ow5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5fmnlwSG6vq"
      },
      "source": [
        "Let's train our new model with the data augmentation layer. We will need to train for more epochs, so that our network has better chance of seeing all the original images (since now we cannot guarantee that our original non-augmented image is seen by our model every epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1eWVr6WKuo9"
      },
      "source": [
        "### Note: the training will take quite a while. We have previously trained the model for 100-epochs.\n",
        "### You can download the checkpoints by uncommenting the following and skip the next cell \"mode.fit()\"\n",
        "\n",
        "# !wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/checkpoints/week3-1-100epochs.zip\n",
        "# !unzip week3-1-100epochs.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv2sEKTrYl81"
      },
      "source": [
        "## Comment out this if you just want to use the pretrained weights\n",
        "def create_tb_callback(): \n",
        "\n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "        import time\n",
        "        \n",
        "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "        return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, \n",
        "          epochs=100, \n",
        "          callbacks=[create_tb_callback(), model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI1D6tyc-HPT"
      },
      "source": [
        "model.load_weights(\"best_checkpoint\")\n",
        "model.evaluate(val_ds)\n",
        "\n",
        "# saving of model with augmentation layer throws exception in TF2.7, potentially there is a bug in tf2.7\n",
        "# model.save(\"cats_dogs_augmented_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZY8r1O-vYG"
      },
      "source": [
        "Let's visualize our training using Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LZDuZaW-vYG"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLypNoy-vYG"
      },
      "source": [
        "Thanks to data augmentation, our model has less overfitting now, as the training curves are more closely tracking the validation \n",
        "curves. We are now able to reach an validation accuracy of about 80%, slightly better than previously.\n",
        "\n",
        "However, it would be very difficult to improve the model any further even with data augmentation. The augmented images are still heavily correlated, since they come from a small number of original images -- we cannot produce new information, we can only remix existing information. As next step to improve our accuracy on this problem, we will have to leverage transfer learning using pre-trained model, which will be the focus of next exercises."
      ]
    }
  ]
}