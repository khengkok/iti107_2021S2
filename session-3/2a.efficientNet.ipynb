{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.comgithub/nyp-sit/sdaai-iti107/blob/main/session-3/improved_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved model using Transfer Learning\n",
    "\n",
    "Welcome to this week's programming exercise. In this exercise, we use transfer learning to improve our baseline model. We make use of a model (VGG19) that is already trained on ImageNet and use the convolutional neural network as a feature extractor and train a classifier specifically for our emotion classification task.\n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- understand how to load a pretrained model with and without the classification layer  \n",
    "- extract training features using the pre-trained model as feature extractor\n",
    "- train a classifier using the extracted features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning involved using the knowledge learnt in another network (that is trained on large dataset) for some other similar task and transfer that to a new task. There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. Let's start with feature extraction approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "In this approach, we only take the convolutional base of pretrained models and use it to extract features from the images, and use the extracted features as input features to train a classifier. \n",
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/resources/swapping_fc_classifier.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained Model as Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using VGG19 as our pretrained model (you can choose any other pretrained model, such as ResNet, etc). Keras comes with a set of [pretrained models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) you can choose from. In the following call, we load the model VGG19 without including the classification layers (`include_top=False`). In the weights, we specify that we want to download the weights that was trained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this to larger or smaller size\n",
    "img_height, img_width = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-22 14:11:29.796327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:29.806478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:29.807145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:29.808627: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-22 14:11:29.810104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:29.810712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:29.811108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:30.408343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:30.408829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:30.408939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-10-22 14:11:30.409335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-22 14:11:30.409517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2121 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rescaling (Rescaling)           (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 224, 224, 3)  7           rescaling[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv_pad (ZeroPadding2D)   (None, 225, 225, 3)  0           normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv (Conv2D)              (None, 112, 112, 32) 864         stem_conv_pad[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn (BatchNormalization)    (None, 112, 112, 32) 128         stem_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "stem_activation (Activation)    (None, 112, 112, 32) 0           stem_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1a_dwconv (DepthwiseConv2D (None, 112, 112, 32) 288         stem_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1a_bn (BatchNormalization) (None, 112, 112, 32) 128         block1a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block1a_activation (Activation) (None, 112, 112, 32) 0           block1a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_squeeze (GlobalAvera (None, 32)           0           block1a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reshape (Reshape)    (None, 1, 1, 32)     0           block1a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reduce (Conv2D)      (None, 1, 1, 8)      264         block1a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_expand (Conv2D)      (None, 1, 1, 32)     288         block1a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_excite (Multiply)    (None, 112, 112, 32) 0           block1a_activation[0][0]         \n",
      "                                                                 block1a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_conv (Conv2D)   (None, 112, 112, 16) 512         block1a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_bn (BatchNormal (None, 112, 112, 16) 64          block1a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_conv (Conv2D)    (None, 112, 112, 96) 1536        block1a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_bn (BatchNormali (None, 112, 112, 96) 384         block2a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_activation (Acti (None, 112, 112, 96) 0           block2a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_dwconv_pad (ZeroPadding (None, 113, 113, 96) 0           block2a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2a_dwconv (DepthwiseConv2D (None, 56, 56, 96)   864         block2a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_bn (BatchNormalization) (None, 56, 56, 96)   384         block2a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2a_activation (Activation) (None, 56, 56, 96)   0           block2a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_squeeze (GlobalAvera (None, 96)           0           block2a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reshape (Reshape)    (None, 1, 1, 96)     0           block2a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reduce (Conv2D)      (None, 1, 1, 4)      388         block2a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_expand (Conv2D)      (None, 1, 1, 96)     480         block2a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_excite (Multiply)    (None, 56, 56, 96)   0           block2a_activation[0][0]         \n",
      "                                                                 block2a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_conv (Conv2D)   (None, 56, 56, 24)   2304        block2a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_bn (BatchNormal (None, 56, 56, 24)   96          block2a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_conv (Conv2D)    (None, 56, 56, 144)  3456        block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_bn (BatchNormali (None, 56, 56, 144)  576         block2b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_activation (Acti (None, 56, 56, 144)  0           block2b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_dwconv (DepthwiseConv2D (None, 56, 56, 144)  1296        block2b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2b_bn (BatchNormalization) (None, 56, 56, 144)  576         block2b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2b_activation (Activation) (None, 56, 56, 144)  0           block2b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_squeeze (GlobalAvera (None, 144)          0           block2b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reshape (Reshape)    (None, 1, 1, 144)    0           block2b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block2b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block2b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_excite (Multiply)    (None, 56, 56, 144)  0           block2b_activation[0][0]         \n",
      "                                                                 block2b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_conv (Conv2D)   (None, 56, 56, 24)   3456        block2b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_bn (BatchNormal (None, 56, 56, 24)   96          block2b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_drop (Dropout)          (None, 56, 56, 24)   0           block2b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_add (Add)               (None, 56, 56, 24)   0           block2b_drop[0][0]               \n",
      "                                                                 block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_conv (Conv2D)    (None, 56, 56, 144)  3456        block2b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_bn (BatchNormali (None, 56, 56, 144)  576         block3a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_activation (Acti (None, 56, 56, 144)  0           block3a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_dwconv_pad (ZeroPadding (None, 59, 59, 144)  0           block3a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3a_dwconv (DepthwiseConv2D (None, 28, 28, 144)  3600        block3a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_bn (BatchNormalization) (None, 28, 28, 144)  576         block3a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3a_activation (Activation) (None, 28, 28, 144)  0           block3a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_squeeze (GlobalAvera (None, 144)          0           block3a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reshape (Reshape)    (None, 1, 1, 144)    0           block3a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block3a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block3a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_excite (Multiply)    (None, 28, 28, 144)  0           block3a_activation[0][0]         \n",
      "                                                                 block3a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_conv (Conv2D)   (None, 28, 28, 40)   5760        block3a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_bn (BatchNormal (None, 28, 28, 40)   160         block3a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_conv (Conv2D)    (None, 28, 28, 240)  9600        block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_bn (BatchNormali (None, 28, 28, 240)  960         block3b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_activation (Acti (None, 28, 28, 240)  0           block3b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_dwconv (DepthwiseConv2D (None, 28, 28, 240)  6000        block3b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3b_bn (BatchNormalization) (None, 28, 28, 240)  960         block3b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3b_activation (Activation) (None, 28, 28, 240)  0           block3b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_squeeze (GlobalAvera (None, 240)          0           block3b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reshape (Reshape)    (None, 1, 1, 240)    0           block3b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block3b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block3b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_excite (Multiply)    (None, 28, 28, 240)  0           block3b_activation[0][0]         \n",
      "                                                                 block3b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_conv (Conv2D)   (None, 28, 28, 40)   9600        block3b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_bn (BatchNormal (None, 28, 28, 40)   160         block3b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_drop (Dropout)          (None, 28, 28, 40)   0           block3b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_add (Add)               (None, 28, 28, 40)   0           block3b_drop[0][0]               \n",
      "                                                                 block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_conv (Conv2D)    (None, 28, 28, 240)  9600        block3b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_bn (BatchNormali (None, 28, 28, 240)  960         block4a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_activation (Acti (None, 28, 28, 240)  0           block4a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_dwconv_pad (ZeroPadding (None, 29, 29, 240)  0           block4a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4a_dwconv (DepthwiseConv2D (None, 14, 14, 240)  2160        block4a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_bn (BatchNormalization) (None, 14, 14, 240)  960         block4a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4a_activation (Activation) (None, 14, 14, 240)  0           block4a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_squeeze (GlobalAvera (None, 240)          0           block4a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reshape (Reshape)    (None, 1, 1, 240)    0           block4a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block4a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block4a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_excite (Multiply)    (None, 14, 14, 240)  0           block4a_activation[0][0]         \n",
      "                                                                 block4a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_conv (Conv2D)   (None, 14, 14, 80)   19200       block4a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_bn (BatchNormal (None, 14, 14, 80)   320         block4a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_conv (Conv2D)    (None, 14, 14, 480)  38400       block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_bn (BatchNormali (None, 14, 14, 480)  1920        block4b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_activation (Acti (None, 14, 14, 480)  0           block4b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_dwconv (DepthwiseConv2D (None, 14, 14, 480)  4320        block4b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4b_bn (BatchNormalization) (None, 14, 14, 480)  1920        block4b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4b_activation (Activation) (None, 14, 14, 480)  0           block4b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_squeeze (GlobalAvera (None, 480)          0           block4b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_excite (Multiply)    (None, 14, 14, 480)  0           block4b_activation[0][0]         \n",
      "                                                                 block4b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_conv (Conv2D)   (None, 14, 14, 80)   38400       block4b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_bn (BatchNormal (None, 14, 14, 80)   320         block4b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_drop (Dropout)          (None, 14, 14, 80)   0           block4b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_add (Add)               (None, 14, 14, 80)   0           block4b_drop[0][0]               \n",
      "                                                                 block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_conv (Conv2D)    (None, 14, 14, 480)  38400       block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_bn (BatchNormali (None, 14, 14, 480)  1920        block4c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_activation (Acti (None, 14, 14, 480)  0           block4c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_dwconv (DepthwiseConv2D (None, 14, 14, 480)  4320        block4c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4c_bn (BatchNormalization) (None, 14, 14, 480)  1920        block4c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4c_activation (Activation) (None, 14, 14, 480)  0           block4c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_squeeze (GlobalAvera (None, 480)          0           block4c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_excite (Multiply)    (None, 14, 14, 480)  0           block4c_activation[0][0]         \n",
      "                                                                 block4c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_conv (Conv2D)   (None, 14, 14, 80)   38400       block4c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_bn (BatchNormal (None, 14, 14, 80)   320         block4c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4c_drop (Dropout)          (None, 14, 14, 80)   0           block4c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_add (Add)               (None, 14, 14, 80)   0           block4c_drop[0][0]               \n",
      "                                                                 block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_conv (Conv2D)    (None, 14, 14, 480)  38400       block4c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_bn (BatchNormali (None, 14, 14, 480)  1920        block5a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_activation (Acti (None, 14, 14, 480)  0           block5a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_dwconv (DepthwiseConv2D (None, 14, 14, 480)  12000       block5a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5a_bn (BatchNormalization) (None, 14, 14, 480)  1920        block5a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5a_activation (Activation) (None, 14, 14, 480)  0           block5a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_squeeze (GlobalAvera (None, 480)          0           block5a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_reshape (Reshape)    (None, 1, 1, 480)    0           block5a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block5a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block5a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_excite (Multiply)    (None, 14, 14, 480)  0           block5a_activation[0][0]         \n",
      "                                                                 block5a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_project_conv (Conv2D)   (None, 14, 14, 112)  53760       block5a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_project_bn (BatchNormal (None, 14, 14, 112)  448         block5a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_conv (Conv2D)    (None, 14, 14, 672)  75264       block5a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_bn (BatchNormali (None, 14, 14, 672)  2688        block5b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_activation (Acti (None, 14, 14, 672)  0           block5b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_dwconv (DepthwiseConv2D (None, 14, 14, 672)  16800       block5b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5b_bn (BatchNormalization) (None, 14, 14, 672)  2688        block5b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5b_activation (Activation) (None, 14, 14, 672)  0           block5b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_squeeze (GlobalAvera (None, 672)          0           block5b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_excite (Multiply)    (None, 14, 14, 672)  0           block5b_activation[0][0]         \n",
      "                                                                 block5b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_project_conv (Conv2D)   (None, 14, 14, 112)  75264       block5b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_project_bn (BatchNormal (None, 14, 14, 112)  448         block5b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5b_drop (Dropout)          (None, 14, 14, 112)  0           block5b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_add (Add)               (None, 14, 14, 112)  0           block5b_drop[0][0]               \n",
      "                                                                 block5a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_conv (Conv2D)    (None, 14, 14, 672)  75264       block5b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_bn (BatchNormali (None, 14, 14, 672)  2688        block5c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_activation (Acti (None, 14, 14, 672)  0           block5c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_dwconv (DepthwiseConv2D (None, 14, 14, 672)  16800       block5c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5c_bn (BatchNormalization) (None, 14, 14, 672)  2688        block5c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5c_activation (Activation) (None, 14, 14, 672)  0           block5c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_squeeze (GlobalAvera (None, 672)          0           block5c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_excite (Multiply)    (None, 14, 14, 672)  0           block5c_activation[0][0]         \n",
      "                                                                 block5c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_project_conv (Conv2D)   (None, 14, 14, 112)  75264       block5c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_project_bn (BatchNormal (None, 14, 14, 112)  448         block5c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5c_drop (Dropout)          (None, 14, 14, 112)  0           block5c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_add (Add)               (None, 14, 14, 112)  0           block5c_drop[0][0]               \n",
      "                                                                 block5b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_conv (Conv2D)    (None, 14, 14, 672)  75264       block5c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_bn (BatchNormali (None, 14, 14, 672)  2688        block6a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_activation (Acti (None, 14, 14, 672)  0           block6a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_dwconv_pad (ZeroPadding (None, 17, 17, 672)  0           block6a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6a_dwconv (DepthwiseConv2D (None, 7, 7, 672)    16800       block6a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_bn (BatchNormalization) (None, 7, 7, 672)    2688        block6a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6a_activation (Activation) (None, 7, 7, 672)    0           block6a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_squeeze (GlobalAvera (None, 672)          0           block6a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_reshape (Reshape)    (None, 1, 1, 672)    0           block6a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block6a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block6a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_excite (Multiply)    (None, 7, 7, 672)    0           block6a_activation[0][0]         \n",
      "                                                                 block6a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_project_conv (Conv2D)   (None, 7, 7, 192)    129024      block6a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_project_bn (BatchNormal (None, 7, 7, 192)    768         block6a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_conv (Conv2D)    (None, 7, 7, 1152)   221184      block6a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_bn (BatchNormali (None, 7, 7, 1152)   4608        block6b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_activation (Acti (None, 7, 7, 1152)   0           block6b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_dwconv (DepthwiseConv2D (None, 7, 7, 1152)   28800       block6b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6b_bn (BatchNormalization) (None, 7, 7, 1152)   4608        block6b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6b_activation (Activation) (None, 7, 7, 1152)   0           block6b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_squeeze (GlobalAvera (None, 1152)         0           block6b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_excite (Multiply)    (None, 7, 7, 1152)   0           block6b_activation[0][0]         \n",
      "                                                                 block6b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_project_conv (Conv2D)   (None, 7, 7, 192)    221184      block6b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_project_bn (BatchNormal (None, 7, 7, 192)    768         block6b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6b_drop (Dropout)          (None, 7, 7, 192)    0           block6b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_add (Add)               (None, 7, 7, 192)    0           block6b_drop[0][0]               \n",
      "                                                                 block6a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_conv (Conv2D)    (None, 7, 7, 1152)   221184      block6b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_bn (BatchNormali (None, 7, 7, 1152)   4608        block6c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_activation (Acti (None, 7, 7, 1152)   0           block6c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_dwconv (DepthwiseConv2D (None, 7, 7, 1152)   28800       block6c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6c_bn (BatchNormalization) (None, 7, 7, 1152)   4608        block6c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6c_activation (Activation) (None, 7, 7, 1152)   0           block6c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_squeeze (GlobalAvera (None, 1152)         0           block6c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_excite (Multiply)    (None, 7, 7, 1152)   0           block6c_activation[0][0]         \n",
      "                                                                 block6c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_project_conv (Conv2D)   (None, 7, 7, 192)    221184      block6c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_project_bn (BatchNormal (None, 7, 7, 192)    768         block6c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6c_drop (Dropout)          (None, 7, 7, 192)    0           block6c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_add (Add)               (None, 7, 7, 192)    0           block6c_drop[0][0]               \n",
      "                                                                 block6b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_conv (Conv2D)    (None, 7, 7, 1152)   221184      block6c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_bn (BatchNormali (None, 7, 7, 1152)   4608        block6d_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_activation (Acti (None, 7, 7, 1152)   0           block6d_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_dwconv (DepthwiseConv2D (None, 7, 7, 1152)   28800       block6d_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6d_bn (BatchNormalization) (None, 7, 7, 1152)   4608        block6d_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6d_activation (Activation) (None, 7, 7, 1152)   0           block6d_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_squeeze (GlobalAvera (None, 1152)         0           block6d_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6d_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6d_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6d_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_excite (Multiply)    (None, 7, 7, 1152)   0           block6d_activation[0][0]         \n",
      "                                                                 block6d_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_project_conv (Conv2D)   (None, 7, 7, 192)    221184      block6d_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_project_bn (BatchNormal (None, 7, 7, 192)    768         block6d_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6d_drop (Dropout)          (None, 7, 7, 192)    0           block6d_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_add (Add)               (None, 7, 7, 192)    0           block6d_drop[0][0]               \n",
      "                                                                 block6c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_conv (Conv2D)    (None, 7, 7, 1152)   221184      block6d_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_bn (BatchNormali (None, 7, 7, 1152)   4608        block7a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_activation (Acti (None, 7, 7, 1152)   0           block7a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_dwconv (DepthwiseConv2D (None, 7, 7, 1152)   10368       block7a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block7a_bn (BatchNormalization) (None, 7, 7, 1152)   4608        block7a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block7a_activation (Activation) (None, 7, 7, 1152)   0           block7a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_squeeze (GlobalAvera (None, 1152)         0           block7a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block7a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block7a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block7a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_excite (Multiply)    (None, 7, 7, 1152)   0           block7a_activation[0][0]         \n",
      "                                                                 block7a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_project_conv (Conv2D)   (None, 7, 7, 320)    368640      block7a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_project_bn (BatchNormal (None, 7, 7, 320)    1280        block7a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "top_conv (Conv2D)               (None, 7, 7, 1280)   409600      block7a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "top_bn (BatchNormalization)     (None, 7, 7, 1280)   5120        top_conv[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "top_activation (Activation)     (None, 7, 7, 1280)   0           top_bn[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 4,007,548\n",
      "Non-trainable params: 42,023\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# base_model = tf.keras.applications.VGG16(input_shape=(img_height, img_width) + (3,),\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "base_model = EfficientNetB0(input_shape=(img_height, img_width) + (3,), include_top=False, weights='imagenet')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Examine the print out from `model.summary()`\n",
    "- What is the last layer in the pretrained model and what is the output shape? Do you have any Fully connected layers?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "The last layer is the MaxPooling2D layer. The output is a 512 feature maps of 4x4 size. There is no Fully connected (Dense) layers. The network is a convolutional base network.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/intel_emotions_dataset.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('intel_emotions_dataset.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
    "dataset_dir = os.path.dirname(path_to_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will setup our training and validation dataset as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1631 files belonging to 2 classes.\n",
      "Using 1305 files for training.\n",
      "Found 1631 files belonging to 2 classes.\n",
      "Using 326 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "image_size = (img_height, img_width)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary'\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features on the train set \n",
    "\n",
    "We use `predict()` to loop through all the train images (and also the validation images). We can also pass the images directly to the keras model, e.g. `model(images)`. The output will be the features spit out by the convolutional base. We will then use these features as our training samples instead of the original images.\n",
    "\n",
    "However, before we pass the images through the convolutional base, it is IMPORTANT to pre-process the image using the model-specific preprocessing function. Many people *FORGOT* about this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve the preprocess_input function of vgg16 for use later \n",
    "# preprocess_input_fn = vgg16.preprocess_input\n",
    "import tensorflow.keras.applications.efficientnet as efficientnet\n",
    "preprocess_input_fn = efficientnet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "def get_features_labels(dataset): \n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in dataset:   # each iteration yields a batch of images\n",
    "        # pre-process the features\n",
    "        preprocessed_images = preprocess_input_fn(images)\n",
    "        features = base_model(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # concatenate the features from all the batches\n",
    "    all_features, all_labels = np.concatenate(all_features), np.concatenate(all_labels)\n",
    "    \n",
    "    return all_features, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call the extract function for both training dataset and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_features_labels(train_ds)\n",
    "X_val, y_val = get_features_labels(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the features\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now save the features to local storage, as numpy arrays. We will load these features later on to be used for training our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model\n",
    "\n",
    "Now we will build a new model that takes in the extracted features as input. Instead of the usual flatten layer, followed by dense layers, let us use a GAP layer, followed by Dense, a Droput and another Dense that output the prediction. \n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. What should be input shape to our model? \n",
    "2. What is the output shape of the Global Average Pooling (GAP) layer? \n",
    "3. How many units we need for output, and what should we use as activation function? \n",
    "\n",
    "Complete the code below. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. The input shape should be (4, 4, 512) which is the output shape of our convolutional base\n",
    "2. The output shape of GAP is (512) since the maxpooling layer (the last layer) of the convolutional base has 512 feature maps (channels). \n",
    "3. We need only 1 output unit as we are doing binary classification (0 or 1) and we should use 'sigmoid' as the activation function for binary classification. \n",
    "\n",
    "Codes: \n",
    "\n",
    "```python\n",
    "inputs = layers.Input(shape=X_train.shape[1:])\n",
    "x = layers.GlobalAveragePooling2D()(inputs)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "model_top.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "``` \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model here, you can use either Keras Sequential or functional API to build your model\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "## TODO: build your layers here, include the input and output layer\n",
    "\n",
    "inputs = layers.Input(shape=X_train.shape[1:])\n",
    "x = layers.GlobalAveragePooling2D()(inputs)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# define the inputs and outputs of the model \n",
    "\n",
    "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "model_top.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "### END YOUR CODE HERE ###    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our classifier we the extracted features (X_train) for 100 epochs. The training will be fast, as we only have very few parameters (around 200k) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "41/41 [==============================] - 1s 11ms/step - loss: 0.6250 - accuracy: 0.6851 - val_loss: 0.5642 - val_accuracy: 0.7301\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.4748 - accuracy: 0.7801 - val_loss: 0.5561 - val_accuracy: 0.7485\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.4182 - accuracy: 0.8123 - val_loss: 0.5417 - val_accuracy: 0.7485\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.3689 - accuracy: 0.8414 - val_loss: 0.5293 - val_accuracy: 0.7362\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.3339 - accuracy: 0.8536 - val_loss: 0.5172 - val_accuracy: 0.7423\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.2970 - accuracy: 0.8628 - val_loss: 0.5509 - val_accuracy: 0.7331\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.2753 - accuracy: 0.8828 - val_loss: 0.5326 - val_accuracy: 0.7761\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.2844 - accuracy: 0.8805 - val_loss: 0.5077 - val_accuracy: 0.7546\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.2667 - accuracy: 0.8889 - val_loss: 0.5400 - val_accuracy: 0.7546\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.2370 - accuracy: 0.9027 - val_loss: 0.5245 - val_accuracy: 0.7699\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2229 - accuracy: 0.9096 - val_loss: 0.5229 - val_accuracy: 0.7515\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.2239 - accuracy: 0.9088 - val_loss: 0.5223 - val_accuracy: 0.7730\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1921 - accuracy: 0.9272 - val_loss: 0.5527 - val_accuracy: 0.7638\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1836 - accuracy: 0.9249 - val_loss: 0.5612 - val_accuracy: 0.7791\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1827 - accuracy: 0.9318 - val_loss: 0.5373 - val_accuracy: 0.7546\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1669 - accuracy: 0.9410 - val_loss: 0.5448 - val_accuracy: 0.7546\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.9517 - val_loss: 0.5553 - val_accuracy: 0.7393\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1464 - accuracy: 0.9487 - val_loss: 0.5817 - val_accuracy: 0.7546\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1568 - accuracy: 0.9410 - val_loss: 0.5969 - val_accuracy: 0.7546\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1384 - accuracy: 0.9586 - val_loss: 0.5852 - val_accuracy: 0.7730\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1300 - accuracy: 0.9510 - val_loss: 0.5919 - val_accuracy: 0.7607\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1217 - accuracy: 0.9571 - val_loss: 0.6154 - val_accuracy: 0.7546\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.1337 - accuracy: 0.9510 - val_loss: 0.5786 - val_accuracy: 0.7423\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1059 - accuracy: 0.9594 - val_loss: 0.5987 - val_accuracy: 0.7454\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9648 - val_loss: 0.5831 - val_accuracy: 0.7577\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0882 - accuracy: 0.9693 - val_loss: 0.6382 - val_accuracy: 0.7454\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0990 - accuracy: 0.9648 - val_loss: 0.5978 - val_accuracy: 0.7393\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0930 - accuracy: 0.9670 - val_loss: 0.6179 - val_accuracy: 0.7607\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0942 - accuracy: 0.9663 - val_loss: 0.6081 - val_accuracy: 0.7485\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0746 - accuracy: 0.9770 - val_loss: 0.6319 - val_accuracy: 0.7515\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 0.6358 - val_accuracy: 0.7638\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0675 - accuracy: 0.9762 - val_loss: 0.6897 - val_accuracy: 0.7577\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9732 - val_loss: 0.6972 - val_accuracy: 0.7577\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0665 - accuracy: 0.9778 - val_loss: 0.7220 - val_accuracy: 0.7393\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0629 - accuracy: 0.9770 - val_loss: 0.6838 - val_accuracy: 0.7546\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0641 - accuracy: 0.9785 - val_loss: 0.6432 - val_accuracy: 0.7423\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0763 - accuracy: 0.9709 - val_loss: 0.6563 - val_accuracy: 0.7454\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0612 - accuracy: 0.9808 - val_loss: 0.6788 - val_accuracy: 0.7454\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.6767 - val_accuracy: 0.7638\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 0.7236 - val_accuracy: 0.7454\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0542 - accuracy: 0.9808 - val_loss: 0.6885 - val_accuracy: 0.7485\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0533 - accuracy: 0.9801 - val_loss: 0.6905 - val_accuracy: 0.7577\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0546 - accuracy: 0.9847 - val_loss: 0.7027 - val_accuracy: 0.7607\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0601 - accuracy: 0.9824 - val_loss: 0.6788 - val_accuracy: 0.7699\n",
      "Epoch 45/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.9824 - val_loss: 0.7111 - val_accuracy: 0.7791\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9778 - val_loss: 0.7096 - val_accuracy: 0.7730\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0635 - accuracy: 0.9793 - val_loss: 0.7218 - val_accuracy: 0.7515\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0458 - accuracy: 0.9870 - val_loss: 0.7255 - val_accuracy: 0.7607\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 0.9885 - val_loss: 0.7270 - val_accuracy: 0.7546\n",
      "Epoch 50/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 0.9877 - val_loss: 0.7381 - val_accuracy: 0.7546\n",
      "Epoch 51/100\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.7843 - val_accuracy: 0.7515\n",
      "Epoch 52/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.7730 - val_accuracy: 0.7485\n",
      "Epoch 53/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 0.7496 - val_accuracy: 0.7607\n",
      "Epoch 54/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.7586 - val_accuracy: 0.7423\n",
      "Epoch 55/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0523 - accuracy: 0.9831 - val_loss: 0.7822 - val_accuracy: 0.7638\n",
      "Epoch 56/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0518 - accuracy: 0.9839 - val_loss: 0.7980 - val_accuracy: 0.7638\n",
      "Epoch 57/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.8035 - val_accuracy: 0.7485\n",
      "Epoch 58/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0414 - accuracy: 0.9893 - val_loss: 0.8146 - val_accuracy: 0.7485\n",
      "Epoch 59/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9862 - val_loss: 0.8491 - val_accuracy: 0.7331\n",
      "Epoch 60/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0358 - accuracy: 0.9893 - val_loss: 0.8223 - val_accuracy: 0.7331\n",
      "Epoch 61/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0355 - accuracy: 0.9900 - val_loss: 0.8231 - val_accuracy: 0.7393\n",
      "Epoch 62/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0417 - accuracy: 0.9847 - val_loss: 0.8666 - val_accuracy: 0.7209\n",
      "Epoch 63/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0420 - accuracy: 0.9824 - val_loss: 0.7931 - val_accuracy: 0.7331\n",
      "Epoch 64/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9916 - val_loss: 0.8319 - val_accuracy: 0.7362\n",
      "Epoch 65/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.9877 - val_loss: 0.8225 - val_accuracy: 0.7423\n",
      "Epoch 66/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0389 - accuracy: 0.9900 - val_loss: 0.8590 - val_accuracy: 0.7454\n",
      "Epoch 67/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 0.8513 - val_accuracy: 0.7270\n",
      "Epoch 68/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 0.9854 - val_loss: 0.8276 - val_accuracy: 0.7515\n",
      "Epoch 69/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0396 - accuracy: 0.9816 - val_loss: 0.8113 - val_accuracy: 0.7546\n",
      "Epoch 70/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9885 - val_loss: 0.8373 - val_accuracy: 0.7515\n",
      "Epoch 71/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0437 - accuracy: 0.9824 - val_loss: 0.8221 - val_accuracy: 0.7393\n",
      "Epoch 72/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9870 - val_loss: 0.8297 - val_accuracy: 0.7577\n",
      "Epoch 73/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.9908 - val_loss: 0.8042 - val_accuracy: 0.7638\n",
      "Epoch 74/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 0.9877 - val_loss: 0.8311 - val_accuracy: 0.7669\n",
      "Epoch 75/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0459 - accuracy: 0.9862 - val_loss: 0.8236 - val_accuracy: 0.7669\n",
      "Epoch 76/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.9893 - val_loss: 0.8078 - val_accuracy: 0.7607\n",
      "Epoch 77/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 0.9885 - val_loss: 0.8569 - val_accuracy: 0.7607\n",
      "Epoch 78/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9870 - val_loss: 0.8514 - val_accuracy: 0.7485\n",
      "Epoch 79/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0426 - accuracy: 0.9831 - val_loss: 0.8574 - val_accuracy: 0.7546\n",
      "Epoch 80/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.8310 - val_accuracy: 0.7577\n",
      "Epoch 81/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0595 - accuracy: 0.9808 - val_loss: 0.8919 - val_accuracy: 0.7485\n",
      "Epoch 82/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 0.9946 - val_loss: 0.8631 - val_accuracy: 0.7485\n",
      "Epoch 83/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0303 - accuracy: 0.9900 - val_loss: 0.8391 - val_accuracy: 0.7454\n",
      "Epoch 84/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 0.8653 - val_accuracy: 0.7577\n",
      "Epoch 85/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.8773 - val_accuracy: 0.7607\n",
      "Epoch 86/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9854 - val_loss: 0.8576 - val_accuracy: 0.7515\n",
      "Epoch 87/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0233 - accuracy: 0.9931 - val_loss: 0.8901 - val_accuracy: 0.7546\n",
      "Epoch 88/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0277 - accuracy: 0.9916 - val_loss: 0.8474 - val_accuracy: 0.7423\n",
      "Epoch 89/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9854 - val_loss: 0.9494 - val_accuracy: 0.7362\n",
      "Epoch 90/100\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.0251 - accuracy: 0.9931 - val_loss: 0.9184 - val_accuracy: 0.7362\n",
      "Epoch 91/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0232 - accuracy: 0.9916 - val_loss: 0.8654 - val_accuracy: 0.7331\n",
      "Epoch 92/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0238 - accuracy: 0.9946 - val_loss: 0.9190 - val_accuracy: 0.7393\n",
      "Epoch 93/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 0.9862 - val_loss: 0.9040 - val_accuracy: 0.7454\n",
      "Epoch 94/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.9249 - val_accuracy: 0.7454\n",
      "Epoch 95/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0268 - accuracy: 0.9885 - val_loss: 0.9180 - val_accuracy: 0.7791\n",
      "Epoch 96/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.8444 - val_accuracy: 0.7485\n",
      "Epoch 97/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.9900 - val_loss: 0.8636 - val_accuracy: 0.7669\n",
      "Epoch 98/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 0.9893 - val_loss: 0.8935 - val_accuracy: 0.7485\n",
      "Epoch 99/100\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 0.9931 - val_loss: 0.9231 - val_accuracy: 0.7515\n",
      "Epoch 100/100\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.9749 - val_accuracy: 0.7607\n"
     ]
    }
   ],
   "source": [
    "# we will now load the extracted features from the files we save to earlier \n",
    "\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"best_checkpoint\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "hist_top = model_top.fit(X_train, y_train, \n",
    "                         epochs=100, \n",
    "                         validation_data=(X_val, y_val), \n",
    "                         callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.load_weights('best_checkpoint')\n",
    "y_preds = model_top.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.88      0.80       163\n",
      "         1.0       0.85      0.68      0.76       163\n",
      "\n",
      "    accuracy                           0.78       326\n",
      "   macro avg       0.79      0.78      0.78       326\n",
      "weighted avg       0.79      0.78      0.78       326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_preds.flatten() >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0668769 ]\n",
      " [0.551925  ]\n",
      " [0.56629497]\n",
      " [0.1865359 ]\n",
      " [0.935955  ]\n",
      " [0.10884209]\n",
      " [0.9848113 ]\n",
      " [0.25866514]\n",
      " [0.9392409 ]\n",
      " [0.9731554 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an good improvement in the model (should be around 30%). The model also takes much less time to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for deployment\n",
    "\n",
    "We cannot just use our `model_top` that is trained for image classification, as it take extracted features as input, not images. We need to stick back our convolutional base and use an input layer of appropriate shape. This is what we are going to do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "x = preprocess_input_fn(inputs)\n",
    "x = base_model(x)\n",
    "outputs = model_top(x)\n",
    "\n",
    "model_final = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_final.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# inputs = layers.Input(shape=(150, 150, 3))\n",
    "# x = preprocess_input_fn(inputs)\n",
    "# x = conv_base(x)\n",
    "# top_outputs = model_top(x)\n",
    "# model_final = Model(inputs=[inputs], outputs=[top_outputs])\n",
    "# model_final.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop(lr=2e-5), metrics=['acc'])\n",
    "# model_final.summary()\n",
    "# model_final.save(\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, 7, 7, 1280)        4049571   \n",
      "_________________________________________________________________\n",
      "top (Functional)             (None, 1)                 656385    \n",
      "=================================================================\n",
      "Total params: 4,705,956\n",
      "Trainable params: 656,385\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-22 14:12:24.552680: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markk/miniconda3/envs/dlenv/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model_final.save(\"full_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 4s 79ms/step - loss: 0.5612 - accuracy: 0.7791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5611798167228699, 0.7791411280632019]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let just test our full model on the images from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "iterator1 = val_ds.as_numpy_iterator()\n",
    "for images, labels in iterator1:\n",
    "    all_labels.append(labels)\n",
    "    all_images.append(images)\n",
    "\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_images = np.concatenate(all_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_final.predict(all_images)\n",
    "# convert probabilities into classification label based on threshold of 0.5 \n",
    "y_preds = y_pred_probs > 0.5\n",
    "print(len(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.88      0.80       163\n",
      "         1.0       0.85      0.68      0.76       163\n",
      "\n",
      "    accuracy                           0.78       326\n",
      "   macro avg       0.79      0.78      0.78       326\n",
      "weighted avg       0.79      0.78      0.78       326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra exercises\n",
    "\n",
    "Try another pre-trained model such as MobileNetV2 or EfficientNetB0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
