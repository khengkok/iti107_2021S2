{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-8/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKanFNbpTfv4"
      },
      "source": [
        "# Using BERT as Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8phFkS4Tfv7"
      },
      "source": [
        "Other than fine-tuning BERT for downstream task such as text classification, we can use pretrained BERT model as a feature extractor, very much the same as we are using pretrained CNN such as ResNet as feature extractors for downstream task such as image classification and object detection.  \n",
        "\n",
        "In this lab, we will see how we use a pretrained DistilBert Model to extract features (or embedding) from text and use the extracted features (embeddings) to train a classifier to classify text. You can contrast this with the other lab where we train the DistilBert end to end for the classification, and compare the performance of both. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- extract text embeddings from the bert model \n",
        "- use the extracted features for text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nte0QWOZwOAw"
      },
      "source": [
        "## Install Hugging Face Transformers library\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcAO5A0oVMOj",
        "outputId": "0ca6d9f0-7e67-4b65-d6f8-56a308bf1af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiiqcrhLTfv8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh7XepZEZ3Ll"
      },
      "source": [
        "# downloaded the datasets.\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_url)\n",
        "test_df = pd.read_csv(test_data_url)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD78Yn9zw5Yw"
      },
      "source": [
        "The train set has 40000 samples. We will a small subset (e.g. 2000) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model. We use dataframe's sample() to randomly select a subset of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGpIN8KExCOH"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE)\n",
        "test_df = test_df.sample(n=TEST_SIZE)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcdskhQxIcH"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ORi78_VxN3f"
      },
      "source": [
        "train_texts = train_df['review'].to_list()\n",
        "train_labels = train_df['sentiment'].to_list()\n",
        "test_texts = test_df['review'].to_list()\n",
        "test_labels = test_df['sentiment'].to_list()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ikfIcafxSMC"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboqVJWTfwA"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-uncased\".  This is the same as the other lab exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5THnkPITfwA"
      },
      "source": [
        "from transformers import AutoTokenizer \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ltReujTfwA"
      },
      "source": [
        "The pretrained DistilBERT [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) expects a string or list of string, so we need to convert the data frame (or series) into list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFbiO3STfwB"
      },
      "source": [
        "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnqjMMCWTfwB"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgdY5C9TfwB"
      },
      "source": [
        "We will create a tensorflow dataset and use it's efficient batching later to obtain the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Znx8EnTfwB"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HNwF7yYTfwB"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBuGjl7fTfwC"
      },
      "source": [
        "Here we instantiate a pretrained model from 'distilbert-base-cased' and specify output_hidden_state=True so that we get the output from each of the attention layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Pgl3CDTfwC"
      },
      "source": [
        "## Feature Extraction using (Distil)BERT. \n",
        "\n",
        "Here we will load the pretrained model for distibert-based-uncased and use it to extract features from the text (i.e. emeddings). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKSzCCRyTfwC",
        "outputId": "35e9f69c-df9c-4807-ad76-03fc0275edfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\",output_hidden_states=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrWYnDBTfwD"
      },
      "source": [
        "The model will produce two outputs: the 1st output `output[0]` is of shape `(16, 512, 768)` which corresponds to the output of the last hidden layer and the second output `output[1]` is a list of 7 outputs of shape `(16, 512, 768)`, corresponding to the output of each of the 7 attention layers. 768 refers to the hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B793BJtwTfwD"
      },
      "source": [
        "train_embeddings = None\n",
        "\n",
        "for batch in train_dataset:\n",
        "    output = model(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if train_embeddings is None:\n",
        "        train_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        train_embeddings = np.vstack([train_embeddings, sentence_embeddings])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_embeddings = None\n",
        "\n",
        "for batch in val_dataset:\n",
        "    output = model(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if val_embeddings is None:\n",
        "        val_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        val_embeddings = np.vstack([val_embeddings, sentence_embeddings])"
      ],
      "metadata": {
        "id": "pNN5Xgu4EQ7N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5i4L7XTfwD"
      },
      "source": [
        "test_embeddings = None\n",
        "\n",
        "for batch in test_dataset:\n",
        "    output = model(batch[0])\n",
        "    hidden_states = output[1]\n",
        "    # here we take the output of the second last attention layer as our embeddings. \n",
        "    # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "    sentence_embeddings = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "    if test_embeddings is None:\n",
        "        test_embeddings = sentence_embeddings\n",
        "    else:\n",
        "        test_embeddings = np.vstack([test_embeddings, sentence_embeddings])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhAG0WnTfwD"
      },
      "source": [
        "## Train a classifier using the extracted features (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zOWEm85TfwE"
      },
      "source": [
        "X_train = train_embeddings\n",
        "X_val = val_embeddings\n",
        "X_test = test_embeddings\n",
        "y_train = train_labels\n",
        "y_val = val_labels\n",
        "y_test = test_labels"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiA0JMdzTfwE"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhBq-1sTfwE",
        "outputId": "cd77a09b-bbab-4d50-eb1d-73db11fcb8dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'validation score : {clf.score(X_val, y_val)}')\n",
        "print(f'test score : {clf.score(X_test, y_test)}')"
      ],
      "metadata": {
        "id": "wOU0lubfFS6z",
        "outputId": "05974a6e-882c-44bb-f7d4-ee5566fedb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation score : 0.8475\n",
            "test score : 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyaH5WnxTfwE"
      },
      "source": [
        "We should be getting an accuracy score of around 85% which is quite good, considering we are training with only 2000 samples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVDxRUdTfwE"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "1. Modify the code to use the output from a different attention layer as input features (embeddings) to the classifier. \n",
        "2. Modify the code to use BERT model and see if it performs better than the DistilBERT. For BERT Model, the output of different layers are in `output[2]`"
      ]
    }
  ]
}